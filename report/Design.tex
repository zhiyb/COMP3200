\chapter{Design} \label{Chapter:Design}

\section{Specification}

% {\color{red}Specification of works, quantifiable}

\begin{itemize}
	\setlength\itemsep{0em}
	\item Customise a NVIDIA Jetson TK1 embedded board and its Linux operating system for power efficiency.
	\item Interface an OV5647 camera to the target platform through MIPI-CSI interface and V4L2 driver platform.
	\item Configure the camera including different resolution modes through V4L2 interface.
	\item Capture still images and video sequences with the camera through V4L2 interface.
	\item Control the camera frame rate while the camera is actively capturing video stream.
	\item Implement object tracking algorithm on the target platform for video surveillance application.
	\item Optimise the algorithm so that it runs around $30$ FPS on the platform.
	\item Achieve $50\%$ power \mdc{reduction} on some typical real world video surveillance \modc{datasets} through adaptive operation.
	\item Maintaining $80\%$ successful object tracking rate with the adaptive operation.
	\item Build a camera and algorithm power consumption model.
	\item Use the model to estimate power saving achieved through adaptive operation.
\end{itemize}

\section{System block diagram}

\modc{
The block diagram of this adaptive object tracking system is shown in \fref{block}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\columnwidth]{block}
  \caption{Purposed system design block diagram.}
  \label{block}
\end{figure}

The entire system was based on an embedded platform. The application starts with reading video sequence from either a camera through V4L2 interface or sample datasets through OpenCV's media file IO APIs. It can also alter the camera frame interval, or skip video frames in the dataset. The frame read will be delivered to an independent OpenGL video preview thread for displaying on the screen. The frame will also be passed to object tracking algorithms that are based on OpenCV APIs. The algorithms will find moving objects, track them between frames, and render the results on the screen. The tracking results will also be used to predict an optimum frame rate. The desired frame rate will then be configured accordingly, hence the adaptive operation.
}

%{\color{red}Description required}

\section{Hardware}

% {\color{red}Design works, choices of hardware, software, etc.}

\subsection{Hardware platform}

%ARM or x86?

An embedded development board was used in this project, it enables easy power consumption analysis and \mdc{it} is more likely the situation where power efficiency will be required rather than a desktop or server environment.

The most broadly used and powerful embedded processor architecture is ARM. It has relatively high performance grade available, extensively hardware and software support including embedded Linux operating system with reasonable power consumption and \modc{various} power saving modes, therefore was chosen to be used in this project.

Specifically, the Jetson Tegra K1 embedded development platform \cite{NVIDIA:tk1} (\fref{des:board}) featuring a 2.32GHz quad-core ARM CPU and a CUDA enabled Tegra GPU, introduced by NVIDIA, was used in this project. It has a rich set of peripheral interfaces exported \mdc{enabling} low-level control and interfacing with a MIPI-CSI high resolution camera module and is powerful enough to develop programs and execute complex computer vision algorithms on board. The heterogeneous architecture with CUDA enabled GPU can also be fully utilised by the GPU module in the specifically optimised OpenCV library, \mdc{making} it particularly suitable for computer vision tasks.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.7\columnwidth]{board}
  \caption{The Jetson Tegra K1 embedded development board.}
  \label{des:board}
\end{figure}

There are other heterogeneous architecture embedded platforms with ARM CPU cores and ARM GPUs available, but ARM GPUs are currently not supported by OpenCV's GPU module, and the CPUs alone are probably not powerful enough for complex computer vision tasks.

% {\color{red}More}

A general x86 architecture computer running Microsoft Windows operation system with a webcam was also used for algorithm development and remote control of the embedded development board. Since most algorithm developments do not use platform specific features, they can be developed on a general computer then easily migrated to the embedded platform.

\subsection{Camera}

% Specific camera module to be used was not determined yet at the time this progress report was written, but it would be a high resolution camera module from OmniVision \cite{ovt} that can be easily interfaced and directly controlled with the peripheral interfaces on the Jetson TK1 platform. A Linux kernel module driver probably need to be developed in order to control the camera parameters and operations from program running in user space.

The Raspberry Pi camera module (\fref{des:cam}) was used in this project, it features a 5 mega pixels OV5647 camera sensor manufactured by OmniVision Technologies Inc. It is relatively more expensive than buying \mdc{a camera sensor alone}, but the raspberry pi camera module is more broadly available in small quantities and can be easily connected to the board without the need to design a dedicated adapter PCB. It uses the MIPI-CSI interface which the board has native support for, the complete data sheet is also available around the Internet and it features subsampling, frame rate control, auto exposure and white balance functions etc. which are essential to adaptive operation.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.3\columnwidth]{rpi_camera_front}
  \includegraphics[width=0.3\columnwidth]{rpi_camera_back}
  \caption{The Raspberry Pi camera module.}
  \label{des:cam}
\end{figure}

The only difficulty is that the camera sensor can only output frames in raw Bayer pattern format \cite{bayer1976color}, as shown \mdc{in} \fref{bayer}. A typical colourful computer image consists of 3 primary colour channels per pixel, which are red, green and blue, i.e. RGB colour space. However, because of physical and manufacture \mdc{constraints}, most of camera sensors including the camera used in this project have only one intensity sensor per pixel with a specific colour filter, arranged in Bayer pattern format, therefore image processing algorithm \moda{needs} to be applied to convert or approximate the raw data to produce full colour images. Some of the camera sensors have \moda{built-in image processors} that \mdc{apply} the algorithm automatically, but the Raspberry Pi camera sensor used in this project doesn't have that functionality, therefore the algorithm \moda{needs} to be implemented by the application. But since the format can be efficiently converted using the GPU cores and CUDA, that is not a significant issue.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\columnwidth]{bayer}
  \caption{The Bayer pattern pixel arrangement used by the camera module (source from OV5647 datasheet).}
  \label{bayer}
\end{figure}

\section{Software}

\subsection{Operating system}

Ubuntu Linux distribution version 14.04.1 LTS was used on the platform \mdc{and} installed directly from the file system image provided by NVIDIA. Linux is great for this project because it is fully configurable, operating system overheads can be reduced to minimum by disabling unused services, even the graphical desktop environment. Also most Linux operations can be done through just command line interface, possibly via a SSH shell access, therefore programming and controlling the platform can be done anywhere with internet connection, which is very convenient.

The open-source GNU/Linux system also provides familiar and widely used tool sets, kernel driver development is well-documented, cross platform computer vision libraries especially OpenCV \mdc{are} also available \mdc{with} enormous community support.

\subsection{Computer vision API}

OpenCV \cite{opencv} was used to implement the algorithms, due to its cross platform adaptability, \modc{easy development} and large number of efficient algorithms ready for use and analysis. Furthermore, the OpenCV library for Jetson platform provided by NVIDIA had been further optimised, \mdc{providing} 2x-5x speed up \mdc{compared} to regular OpenCV \cite{NVIDIA:perf}. The OpenCV GPU module based on NVIDIA CUDA was also available\mdc{. It} utilises the heterogeneous architecture and provides 5x-20x speed up. These optimisations can reduce computation time dramatically, thus further \mdc{reduce} the power consumption.

\subsection{Testing dataset}

Using a consistent video stream input and a camera power consumption model instead of replaying the same scene in front of the camera is preferred, because reproducing the exactly same video stream from replaying scene is impossible\mdc{. Factors} such as camera and object instability, inaccurate synchronisation, hardware and software restrains will also take effect.

The \modc{CDNET video database} \cite{goyette2012changedetection} was used for testing the algorithms and analysing the performance and accuracy of adaptive operation. The database was rigorous and comprehensive\mdc{. It} involves lots of carefully chosen video streams from different real life scenarios, \mdc{and is} very useful for evaluate object tracking algorithms in different \mdc{situations}.

\section{Algorithms}

\subsection{Object detection algorithm}

The model based object detection algorithms described in previous chapter might be useful for some specific projects, but for a more general object tracking system with previously specified application areas, \moda{motion-based background subtraction} algorithms which can track any kind of moving objects, was used in this project for evaluating adaptive operation.

\moda{Other than background subtraction algorithm, there is another motion-based object detection algorithm described previously, the optical flow algorithm. However, there is a major limitation with it. Since optical flow can only detect moving objects, objects that are moving in a relatively low speed or stops for a few moments, would be undetectable by optical flow, and this scenario is very commonly found.} Therefore background subtraction that can handle this kind of situations was used for the object detection phase.

However, there are several distinctive background subtraction algorithms available for use. After some evaluations, the ViBe algorithm was chosen, because it is relatively efficient, with moderate memory usage and has \moda{GPU implementation that can take the advantages} of heterogeneous platform architecture.

\subsection{Object tracking algorithm}

Connected component analysis is used after foreground masks being extracted from each frame. Specifically, the simple \moda{"find contours"} function that is also used by the simple blob detector from OpenCV was used. It is very simple, but still sufficient to extract blobs from black and white masks without intensive computation.

%Afterwards, the good features to track function is used to determine feature points inside foreground masks, then related to the objects detected respectively.

\moda{Afterwards, the corner points of blobs extracted from foreground masks were used as feature points. Next,} sparse set optical flow will be applied to track the movements of feature points in following frames. By relating the feature points back to \moda{object blobs}, the objects can then be tracked.

% {\color{red}HOW???}
