\chapter{Implementation}

\section{Camera module and driver development}

The Raspberry Pi camera module was connected to the Jetson TK1 board as shown by \fref{imp:schematic}. A 74HCT245 bus transceiver chip was used as a buffer for voltage level conversion, since GPIOs on the Jetson board are using $1.8V$ voltage level while the Raspberry Pi module is working at $3.3V$ voltage level.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\columnwidth]{schematic}
  \caption{Schematic of Jetson board connect to a Raspberry Pi camera module.}
  \label{imp:schematic}
\end{figure}

The camera driver was implemented based on Linux Video for Linux second version (V4L2) framework. It is a great framework that supports varies kinds of different camera sensor models with the same user space interface, across vast number of different embedded Linux platforms, enables cross platform adaptability with accessibility to low level controls.

The V4L2 interface driver on the Jetson platform is composed by 3 different drivers, as shown by \fref{imp:drivers}. Upon system start up, varies camera hardware connection configurations, such as I2C peripheral address and MIPI-CSI data lane information, will be loaded as platform drivers from a board specific initialisation file. Later when soc\_camera driver loads, it will use those informations to try to active corresponding sub-device control drivers, the connection configurations will also be passed to the control drivers. The control drivers can only controls I2C interfaces to access camera registers, while MIPI data lanes and capture buffers are managed by another independent driver.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\columnwidth]{drivers}
  \caption{Structure of V4L2 interface drivers on Jetson board.}
  \label{imp:drivers}
\end{figure}

The soc\_camera top module and capture buffer management module are already provided by NVIDIA, therefore only the platform driver and control driver was required to be implemented.

For adaptive operation, the driver implemented supports different resolution modes available from the camera and precise frame interval alteration. A special control instruction that can change any of the control registers inside the camera sensor was also developed for debugging and controlling purpose.

The GPIO on the camera module is controlling a LED mounted on the module, and it is controllable through the driver. Therefore by controlling the GPIO, it not only can gives indications about camera activity, but also is very useful for debugging and triggering external power analyser.

\section{Application}

% {\color{red}Application structure, multithreading}
% Bayer pattern handling.
% Multi-threading:
% 	i. Camera thread for camera control, buffer swapping, synchronisation and management etc.
% 	ii. OpenGL preview thread shows the camera output in real time, can be suspended for power saving.
% 	iii. OpenCV worker thread running algorithms.
% 	iv. OpenCV viewing thread.
% 	Display results and handle GUI user input, slow, can be suspended for power saving.

\subsection{Bayer pattern handling}

Initially, for capturing still images and OpenGL video preview, the conversion from Bayer pattern to RGB colour space was implemented. Later in OpenCV processing, it was accomplished through OpenCV's built-in functions directly.

To convert a Bayer pattern image to RGB space, a $3 \times 3$ square of pixels around the target pixel is needed. By averaging surrounding pixels according to \fref{imp:bayer4}, RGB values of each pixel can be approximated.

\begin{figure}[H]
  \centering
  \includegraphics[width=\columnwidth]{bayer4}
  \caption{4 different situations in Bayer pattern to RGB colour space conversion.}
  \label{imp:bayer4}
\end{figure}

\fref{imp:bayer_eg} shows a detailed fraction of a test pattern generated by the camera sensor, and Bayer to RGB colour space conversion result.

\begin{figure}[H]
  \centering
  \subfigure [] {
    \includegraphics[width=0.4\columnwidth]{bayer_raw}
    \label{imp:bayer_raw}
  }
  \subfigure [] {
    \includegraphics[width=0.4\columnwidth]{bayer_res}
    \label{imp:bayer_res}
  }
  \caption{Test pattern generated by the camera sensor. \subref{imp:bayer_raw} the generated Bayer pattern, \subref{imp:bayer_res} RGB colour space conversion result.}
  \label{imp:bayer_eg}
\end{figure}

% A typical colourful computer image consists of 3 primary colour channels {\color{red}CITE}, which are red, green and blue. However, because of physical and manufacture constrains, most of camera sensors including the camera used in this project have only one intensity sensor per pixel of a specific colour, arranged in a special pattern called Bayer pattern {\color{red}CITE}, therefore image processing algorithm need to be applied to convert or approximate the raw data to produce full colour images. Some of the camera sensors have a built-in image processor that applies the algorithm automatically, but the camera used in this project doesn't have that functionality, therefore the algorithm need to be implemented by the application.

%{\color{red}Some diagrams show how it is done.}

\subsection{Application structure}

An application was developed to receive video stream from the camera or from a testing dataset, then apply OpenCV processing, real-time input and output video preview as well as adaptive camera control. To take the benefits of the heterogeneous architecture more efficiently and overcome issues with buffer updates, the application was implemented using multi-threading approach with inter-thread synchronisation mechanisms.

The main thread in camera mode is responsible for initialisation, camera control, capture buffer synchronisation and management. It allocates several buffers for the camera driver, assign filled buffers to other processing threads and recycle processed buffers back to camera driver. In dataset mode it reads the sequence of frames, skips frames according to frame rate set by adaptive operation.

After obtained a frame from video stream, an OpenGL live video preview thread was then used to show the frame to a monitor, independent from the processing algorithm. This thread can be independently stalled so that it would not consume any processing power when analysing performances and power consumptions of algorithms. In camera mode, it will convert the frames received from Bayer pattern format to RGB colour space for rendering onto the screen. This process is done by GPU through OpenGL shaders, therefore is extremely fast, around 200 FPS for $640 \times 480$ resolution if synchronisation with main thread is disabled.

The OpenCV algorithms runs on both CPU and GPU, and OpenCV has a constrain that to view the processed images, the application needs to call a user interface update function that is time consuming but not power intensive, therefore 2 threads was used in a pipeline style in order to speed up the processing. The buffers received from main thread will first go through the processing thread, then pass to the second thread for display on the monitor. The display thread can also be stalled for power saving.

%The OpenCV algorithms runs on both CPU and GPU, therefore to take the full advantages of heterogeneous architecture and run the algorithms as fast as possible, the OpenCV processing runs in a pipelined style through 3 threads. Every buffer received will go through first CPU processing thread, then transferred to GPU processing thread, finally .
%and OpenCV has a constrain that to view the processed image results, the application, so that two OpenCV processing threads was created so that.

Finally, an input handing thread for debugging and controlling through command line interface. Most of time this thread is block waiting for user input, therefore won't consume considerable CPU time.

\section{Object detection algorithms}

\subsection{Colour based}

A simple implementation of colour filtering object detection algorithm \cite{MOTBOC.git} was investigated, as shown in \fref{imp:MOTBOC}.

However, this implementation is very limited, it can only detects objects with single colour, cannot distinguish the objects from similar background colour, relies heavily on manually adjusted colour threshold values, very sensitive to the variations of colour from different cameras and environment lighting conditions, therefore not very adaptable. A complex environment may also results into undesired detections, as shown in \fref{imp:MOTBOC_F}. Background objects that has similar colour to the desired red objects were detected as foreground red objects, and some target objects were not been identified and misclassified because of slight change in lighting condition, the objects' colour seen by the camera were no longer inside the colour filters' ranges.

\begin{figure}[H]
  \centering
  \subfigure [] {
    \includegraphics[width=0.4\columnwidth]{MOTBOC_imp}
    \label{imp:MOTBOC_}
  }
  \subfigure [] {
    \includegraphics[width=0.4\columnwidth]{MOTBOC_fail}
    \label{imp:MOTBOC_F}
  }
  \caption{Simple multi object tracking based on colour \cite{MOTBOC.git}. \subref{imp:MOTBOC_} ideal situation, \subref{imp:MOTBOC_F} messed up in a complex environment with slightly different lighting condition.}
  \label{imp:MOTBOC}
\end{figure}

%{\color{red}Descriptions about the failure}

\subsection{Circle detection}

OpenCV's implementation of Hough Circle Transform for circle detection was investigated, as shown by \fref{Figure:circles}.

%\fref{Figure:circles} shows the image processed by circle detection, based on OpenCV's implementation of Hough Circle Transform \cite{opencv:hough_circle}.

\begin{figure}[H]
  \centering
  \subfigure [] {
    \includegraphics[width=0.45\columnwidth]{simple_original}
    \label{Figure:edges:original}
  }
  \subfigure [] {
    \includegraphics[width=0.45\columnwidth]{simple_blur}
    \label{Figure:edges:blur}
  }
  \subfigure [] {
    \includegraphics[width=0.45\columnwidth]{simple_edges}
    \label{Figure:edges:edges}
  }
  \subfigure [] {
    \includegraphics[width=0.45\columnwidth]{simple_circles}
    \label{Figure:edges:circles}
  }
  \caption{Circle detection. \subref{Figure:edges:original} The original image, \subref{Figure:edges:blur} image converted to gray scale and blurred, \subref{Figure:edges:edges} edges extracted, \subref{Figure:edges:circles} circles detected}
  \label{Figure:circles}
\end{figure}

The frame captured from camera (\fref{Figure:edges:original}), was first converted to grey scale then blurred, as shown in \fref{Figure:edges:blur}. Blur or smoothing is necessarily to reduce false object edges that might be detected. Afterwards the object edges in the image was extracted as in \fref{Figure:edges:edges}. Finally \fref{Figure:edges:circles} shows the circles detected by the algorithm.

However, this algorithm is still very limited and inaccurate, as shown by \fref{Figure:circles}, the coin at the top right corner had not been detected, because it appeared to be an ellipse instead of a prefect circle because of camera perspective. Furthermore, in order to detect multiple geometric shapes, different algorithms would be required for each of the different shapes.

\subsection{Cascade Classifier}

% as shown in \fref{Figure:cc_face}

The OpenCV's cascade classifier implementation \cite{opencv:cc} of face and eye detection was investigated as shown in \fref{Figure:cc_face}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\columnwidth]{cc_imp_face}
  \caption{Face and eye detection cascade classifiers, detected face was circled by pink, whereas detected eyes were circled by blue}
  \label{Figure:cc_face}
\end{figure}

Noticeable frame rate drop and latency was experienced (only about 3 fps) when running the cascade classifier implementation on the testing platform, suggests it might not be a suitable algorithm for real-time object tracking application on embedded platforms.

\subsection{Motion based}

4 out of the 5 best algorithms reviewed by the article \cite{bgs:article} that are available in the BGSLibrary as listed in \tref{Table:bgs} were investigated in this project.

%{\color{red}GPU implementation?}

%\iffalse
\begin{table}[H]
  \centering
  \begin{tabular}{cc}
  \toprule
  \textbf{Method ID} & \textbf{Method name}\\
  \midrule
  MultiLayerBGS & Multi-Layer BGS \\
  MixtureOfGaussianV1BGS & Gaussian Mixture Model \\
  LBAdaptiveSOM & Adaptive SOM \\
  DPWrenGABGS & Gaussian Average \\
  \bottomrule
  \end{tabular}
  \caption{Background substraction algorithms investigated (adapted from \cite{bgslibrary})}
  \label{Table:bgs}
\end{table}
%\fi

%The missing Pixel-Based Adaptive Segmenter (PBAS) algorithm , because the algorithm is based on patented ViBE algorithm, therefore removed from the BGSLibrary to avoid patent issue. However,

The ViBE algorithm from early versions of OpenCV as a non-free add-on module implemented with CUDA runs on GPU, which is also the base of the missing PBAS algotihm, was also investigated.

\fref{Figure:bgs_frame} shows the foreground masks obtained from those 4 algorithms through 2 sample frame sequences came with the BGSLibrary \cite{bgslibrary}.

{\color{red}ViBE}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.3\columnwidth]{bgs_frame/input}
  \includegraphics[width=0.3\columnwidth]{bgs_frame/MultiLayerBGS/mask}
  \includegraphics[width=0.3\columnwidth]{bgs_frame/MixtureOfGaussianV1BGS/mask}
  \includegraphics[width=0.3\columnwidth]{bgs_frame/LBAdaptiveSOM/mask}
  \includegraphics[width=0.3\columnwidth]{bgs_frame/DPWrenGABGS/mask}
  \includegraphics[width=0.3\columnwidth]{bgs_frame/ViBE/mask}


  \includegraphics[width=0.3\columnwidth]{bgs_video/input}
  \includegraphics[width=0.3\columnwidth]{bgs_video/MultiLayerBGS/mask}
  \includegraphics[width=0.3\columnwidth]{bgs_video/MixtureOfGaussianV1BGS/mask}
  \includegraphics[width=0.3\columnwidth]{bgs_video/LBAdaptiveSOM/mask}
  \includegraphics[width=0.3\columnwidth]{bgs_video/DPWrenGABGS/mask}
  \includegraphics[width=0.3\columnwidth]{bgs_video/ViBE/mask}

  \caption{Results obtained from background subtraction algorithms. From left to right column: input sample 1, foreground mask obtained, input sample 2, foreground mask obtained. From top to bottom row: MultiLayerBGS, MixtureOfGaussianV1BGS, LBAdaptiveSOM and DPWrenGABGS.}
  \label{Figure:bgs_frame}
\end{figure}

{\color{red}Background truth, how to tell best masks?}

It can be seen from \fref{Figure:bgs_frame} that MultiLayerBGS gave the best foreground masks, but it was also the slowest algorithm on the testing platform.

\section{Object tracking algorithms}

%Connected component analysis or blob detection (\sref{blob}) and movement tracking (\sref{bg:tracking}) algorithms were not implemented yet.

%A suitable camera module need to be ordered and interfaced afterwards, then implement automatic feedback control of frame rate, cropping and down sampling.

\subsection{Connected component analysis}

\subsection{Continuously Adaptive Meanshift}

\subsection{Optical flow}

\section{Adaptive power saving operation}

\section{Video stream dataset}
